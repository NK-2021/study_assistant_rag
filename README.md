AI Study Assistant (RAG)

A local, grounded AI Study Assistant built using a fixed Retrieval-Augmented Generation (RAG) pipeline.
The system answers questions, generates revision notes, and creates MCQs strictly from provided study material, with transparent sources and exports.

------------------------------

âœ¨ Key Features

Question Answering (QA) from notes / PDFs

Exam-focused revision notes

MCQ generation (optional mode)

Strict grounding

Uses only retrieved context

No hallucination

Explicit â€œInsufficient contextâ€ handling

Transparent sources

Shows top-k retrieved chunks

Displays chunk id + distance

Export

Download full result as JSON

Download readable summary as TXT

------------------------------

ğŸ§  Architecture (Locked by Design)

This project intentionally keeps the architecture simple, explicit, and interview-friendly.

![Folder Structure](screenshots/Architecture.png)

------------------------------

âš ï¸ Important:
The following are not changed or abstracted:

Chunking logic

Embedding logic

Vector DB logic

Retrieval logic

Only prompts, modes, and UI behavior vary.
------------------------------

ğŸ§© Tech Stack

Python 3.10+

Streamlit â€“ UI

sentence-transformers (MiniLM) â€“ embeddings

ChromaDB â€“ local vector store

Ollama â€“ local LLM inference

No paid APIs

Offline-friendly
------------------------------

Project Structure

## ğŸ“ Project Structure

![Folder Structure](screenshots/folder-structure.png)

------------------------------

ğŸš€ How to Run
1. Prerequisites

Python installed

Ollama running locally

Model pulled (example):
```ollama pull mistral:7b```

2. Install dependencies
```pip install -r requirements.txt```

3. Run the app
```streamlit run app.py```

Open:
ğŸ‘‰ http://localhost:8501
------------------------------

ğŸ§ª Usage Flow
Step 1: Index Notes

Upload a PDF or

Paste study notes

Click Index Notes

Notes are embedded and stored in Chroma

Step 2: Ask

Select mode:

qa

notes

mcq

Enter question / topic

Click Ask

Step 3: Inspect Results

Answer / notes / MCQs

Evidence quotes from notes

Top-k retrieved chunks with distance

Expandable source view

Step 4: Export

Download JSON (full structured output)

Download TXT (human-readable summary)

ğŸ›¡ï¸ Grounding & Safety Rules

The LLM only sees retrieved chunks

If context is insufficient:

Response explicitly says so

No external knowledge

Output schema enforced via prompt + parser

Evidence is always shown when available

ğŸ¤ Interview Talking Points

â€œThis is a pure RAG system, no fine-tuningâ€

â€œGrounding is enforced at prompt + UI levelâ€

â€œSources are visible to the userâ€

â€œArchitecture is intentionally simple and explainableâ€

â€œRuns fully locally with Ollama + Chromaâ€

ğŸ“¸ Screenshots

Include:

Index screen

Result screen

Sources expanded

Export buttons

âœ… Status

Complete and production-ready for demos & interviews.

No further changes required.









